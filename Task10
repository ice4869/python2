from sklearn.datasets import load_iris
iris = load_iris()
X = iris.data
y = iris.target

#分割訓練集和測試集
from sklearn.model_selection import train_test_split
X_train , X_test , y_train , y_test = train_test_split(X , y , test_size=0.2 , random_state=42 )

#使用分割的資料及訓練模型
from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier()
clf.fit(X_train , y_train) 
y_pred = clf.predict(X_test)

#評估模型表現
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test , y_pred)
print("Accuracy:" , accuracy)

#混淆矩陣
from sklearn.metrics import confusion_matrix

y_true = [0 , 1 , 0 , 1 ]
y_pred = [0 , 1 , 1 , 0 ]

cm = confusion_matrix(y_true , y_pred)
print("Confusion Matrix:\n" , cm)

#準確率(Accuracy)
from sklearn.metrics import accuracy_score

y_true = [0 , 1 , 0 , 1 ]
y_pred = [0 , 1 , 1 , 0 ]

accuracy = accuracy_score(y_true , y_pred)
print("Accuracy:" , accuracy)

from sklearn.metrics import precision_score

y_true = [0 , 1 , 0 , 1 ]
y_pred = [0 , 1 , 1 , 0 ]

precision = precision_score(y_true , y_pred)
print("Precision:" , precision)

#召回率
from sklearn.metrics import recall_score

y_true = [0,1,0,1]
y_pred = [0,1,1,0]

recall = recall_score(y_true , y_pred)
print("Recall:" , recall)

#F1分數(F1 score)
from sklearn.metrics import f1_score

y_true = [0 , 1 , 0 , 1 ]
y_pred = [0 , 1 , 1 , 0 ]

f1 = f1_score(y_true , y_pred)
print("F1 Score:" , f1)

#MAE、MSE、RMSE
from sklearn.metrics import mean_absolute_error,mean_squared_error,mean_squared_error

y_true = [1 , 2 , 3 , 4 , 5 ]
y_pred = [1 , 3 , 2 , 4 , 6 ]

mae = mean_absolute_error(y_true , y_pred)
mse = mean_squared_error(y_true , y_pred)
rmse = mean_squared_error(y_true , y_pred , squared = False)

print("MAE:" , mae)
print("MSE:" , mse)
print("RMSE:" , rmse)

#在 Scikit-Learn 中，我們可以使用 cross_val_score 函數來進行交叉驗證：
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score

#載入鳶尾花數據集
iris = load_iris()

#定義邏輯回歸模型
log_reg = LogisticRegression()

#進行交叉驗證
scores = cross_val_score(log_reg , iris.data , iris.target , cv = 5)

#輸出結果
print("Cross Validation Scores:" , scores)
print("Average Score:" , scores.mean() )

#過度擬合
import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import learning_curve , validation_curve
from sklearn.tree import DecisionTreeClassifier

#生成一個假的二元分類資料集
X,y = make_classification(n_samples = 1000 , n_features = 10 , n_informative = 5,n_redundant = 5 , random_state = 42)

#建立決策樹模型
dt = DecisionTreeClassifier(random_state=42)

#計算學習曲線
train_sizes,train_scores,test_scores = learning_curve(estimator = dt , X =X , y= y , train_sizes = np.linspace(0.1 , 1.0 , 10), cv = 5)

#繪製學習曲線
import matplotlib.pyplot as plt
plt.plot(train_sizes , np.mean(train_scores , axis = 1) , 'o-' , color = 'r' , label = "Training score")
plt.plot(train_sizes , np.mean(test_scores , axis = 1) , 'o-' , color = 'g' , label = "Cross-validation score")
plt.xlabel("Training examples")
plt.ylabel("Score")
plt.legend(loc = "best")
plt.show()

#計算驗證曲線
param_range = np.arange(1, 11)
train_scores, test_scores = validation_curve(estimator = dt , X = X , y= y,param_name = "max_depth" , param_range = param_range , cv = 5)

#繪製驗證曲線
plt.plot(param_range , np.mean(train_scores , axis = 1) , 'o-' , color = 'r' , label = "Training score")
plt.plot(param_range , np.mean(test_scores , axis = 1) , 'o-' , color = 'g' , label = "Cross-validation score")
plt.xlabel("Max depth")
plt.ylabel("Score")
plt.legend(loc = "best")
plt.show()
